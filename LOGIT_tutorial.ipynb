{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SUSY searches using Machine Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/eirikgr/egramsta/postdoc/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from sklearn.preprocessing import Imputer\n",
    "import os.path\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named ROOT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f35525a3fab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEventList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTCut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTH1F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTH2F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTCanvas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkRed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroot_numpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree2array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot2array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'jsroot on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named ROOT"
     ]
    }
   ],
   "source": [
    "from ROOT import gROOT, gDirectory, TFile, TEventList, TCut, TH1F, TH2F, TCanvas, kRed\n",
    "from root_numpy import tree2array, root2array\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define som function to convert the ROOT nTuples into numpy arrays. One possibility is to read from the nTuple each time and make the numpy array. This can be done by using the following function (buildArraysFromROOT()). It takes as input a TTree, variables to print, selection criteria, which events to write and finally name of the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildArraysFromROOT(tree,allowedFeatures,cut,skipEvents,maxEvents,name):\n",
    "    dataContainer = {}\n",
    "    featureNames = []\n",
    "    eventCounter = -1\n",
    "    gROOT.Reset()\n",
    "    \n",
    "    # Get branch names\n",
    "    for item in tree.GetListOfBranches():\n",
    "        featureName = item.GetName()\n",
    "        if featureName in allowedFeatures:\n",
    "            featureNames.append(featureName)\n",
    "            dataContainer[featureName] = []\n",
    "\n",
    "    # Build the event list\n",
    "    tcut = TCut(cut)\n",
    "    tree.Draw(\">>eventList\",tcut)\n",
    "    eventList = TEventList()\n",
    "    eventList = gDirectory.Get(\"eventList\")\n",
    "    nSelectedEvents = eventList.GetN()\n",
    "\n",
    "    # Event loop\n",
    "    for i in range(0,nSelectedEvents):\n",
    "        if (i < skipEvents):\n",
    "            continue\n",
    "        if (i % 100 == 0):\n",
    "            sys.stdout.write(\"Reading %s: %d%%   \\r\" % (tree.GetName(),100*i/(maxEvents+skipEvents)) )\n",
    "            sys.stdout.flush()\n",
    "        if i >= (maxEvents+skipEvents):\n",
    "            break\n",
    "        selectedEvNum = eventList.GetEntry(i)\n",
    "        tree.GetEntry(selectedEvNum)\n",
    "        for feature in featureNames:\n",
    "            dataContainer[feature].append(tree.__getattr__(feature))\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "    # Make the numpy arrays\n",
    "    outputArray = np.array([])\n",
    "    for feature in allowedFeatures:\n",
    "        column = dataContainer[feature]\n",
    "        feature_vector = np.asarray(column)\n",
    "        feature_vector = feature_vector.reshape(feature_vector.size,1)\n",
    "        if outputArray.shape[0]==0:\n",
    "            outputArray = feature_vector\n",
    "        else:\n",
    "            outputArray = np.concatenate((outputArray,feature_vector),axis=1)\n",
    "    imp = Imputer(missing_values=-999, strategy='mean', axis=0)\n",
    "    imp.fit(outputArray)\n",
    "    outputArray = imp.transform(outputArray)\n",
    "    print(name)\n",
    "    print(\"Events: \",outputArray.shape[0])\n",
    "    print(\"Features: \",outputArray.shape[1])\n",
    "    return outputArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if one want to play around with the same input many times it is probably better to write the numpy array into a h5 file once and for all and then read from this file (much faster than converting the ntuple). This can be done in the following function. It takes as input the ROOT file, name of tree in the ROOT file, which variable to write, the name of the output (with directory), the cut to use and finally the chunk size (i.e. how much is stored in memory for each iteration of writing to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeArrays(fileName, treeName, ntupFeatures, outputFileName = \"\", selection = \"\", chunkSize = 1000000):\n",
    "    # ROOT files\n",
    "    print \"Opening ROOT file...\"\n",
    "    if outputFileName:\n",
    "        outputName = outputFileName+\".h5\"\n",
    "    else:\n",
    "        outputName = fileName[:len(fileName)-5]+\".h5\"\n",
    "\n",
    "    inputFile = TFile(fileName)\n",
    "    inputTree = inputFile.Get(treeName)\n",
    "\n",
    "    # Chunks\n",
    "    totalEvents = inputTree.GetEntries()\n",
    "    nChunks = (totalEvents // chunkSize) + 1\n",
    "    remainderSize = totalEvents-(nChunks-1)*chunkSize\n",
    "\n",
    "    # Output file\n",
    "    with h5py.File(outputName, \"w\") as f:\n",
    "\n",
    "       # First chunk (numpy array)\n",
    "       print \"Processing events 0 to\",chunkSize\n",
    "       firstChunk = tree2array(inputTree,start=0,stop=chunkSize,step=1,selection=selection,branches=ntupFeatures)\n",
    "       n = 0\n",
    "       if firstChunk.shape[0] == 0: \n",
    "            while firstChunk.shape[0] == 0:\n",
    "                firstChunk = tree2array(inputTree,start=chunkSize*n,\n",
    "                                            stop=chunkSize*(n+1),step=1,selection=selection,branches=ntupFeatures)\n",
    "                n += 1\n",
    "                print \"INFO \\t Chunk is still empty, trying for the %i%s time \"%(n,\"nd\" if n == 2 else (\"rd\" if n == 3 else (\"st\" if n == 1 else \"th\")))\n",
    "\n",
    "       # Break into columns\n",
    "       firstChunk = np.array(firstChunk[:].tolist())\n",
    "        \n",
    "       #print f.attrs.keys()\n",
    "       #return \n",
    "       # Output data structure\n",
    "       dtype = firstChunk.dtype\n",
    "       row_count = firstChunk.shape[0]\n",
    "       maxshape = (None,) + firstChunk.shape[1:]\n",
    "       dset = f.create_dataset(treeName, shape=firstChunk.shape, maxshape=maxshape,chunks=firstChunk.shape, dtype=dtype)\n",
    "       # Write the first chunk \n",
    "       dset[:] = firstChunk\n",
    "       print type(dset)\n",
    "       print dset.shape\n",
    "       #print dset.attrs.create()\n",
    "\n",
    "       # Loop over chunks\n",
    "       totalEvents_written = firstChunk.shape[0]\n",
    "       for chunk in range(chunkSize if n == 0 else chunkSize*n,nChunks*chunkSize,chunkSize):\n",
    "          start = chunk\n",
    "          stop = start+chunkSize \n",
    "          if (stop>totalEvents): stop = totalEvents \n",
    "          print \"Processing events \",start,\" to \",stop      \n",
    "          # Extraction of data in numpy form, chunk by chunk\n",
    "          chunk = tree2array(inputTree,\n",
    "                             start=start,\n",
    "                             stop=stop,\n",
    "                             step=1,\n",
    "                             selection = selection,\n",
    "                             branches=ntupFeatures)\n",
    "\n",
    "          totalEvents_written += chunk.shape[0]\n",
    "          # Break up into columns\n",
    "          chunk = np.array(chunk[:].tolist())\n",
    "\n",
    "          # Resize the dataset to accommodate the next chunk of rows\n",
    "          dset.resize(row_count + chunk.shape[0], axis=0)\n",
    "\n",
    "          # Write the next chunk\n",
    "          dset[row_count:] = chunk\n",
    "\n",
    "          # Increment the row count\n",
    "          row_count += chunk.shape[0]\n",
    "\n",
    "       # store the variable names for later\n",
    "       f.attrs.create('var_names',ntupFeatures)\n",
    "        \n",
    "    print \"Number of events            = \",totalEvents\n",
    "    print \"Number of events written    = \",totalEvents_written\n",
    "    print \"Number of features          = \",len(ntupFeatures)\n",
    "    print \"Output file name            = \",outputName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next two cells open the root file with an retrieves the TTree. Then we get the name of all the variables available in the nTuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input files and TTrees\n",
    "inputFile = TFile(\"/mn/felt/u1/eirikgr/Downloads/TMVA_tree.root\");\n",
    "tree = inputFile.Get(\"TMVA_tree\")\n",
    "#inputFile = TFile(\"small.root\");\n",
    "#tree = inputFile.Get(\"TMVA_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "susyFeaturesNtup = []\n",
    "listofbr = tree.GetListOfBranches().Clone()\n",
    "listofbr.SetOwner(False)\n",
    "#listofbr.Sort()\n",
    "for br in listofbr:\n",
    "    susyFeaturesNtup.append(br.GetName())\n",
    "print \"List of the %i variables in this ntuple: %s\" %(len(susyFeaturesNtup),\", \".join(susyFeaturesNtup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the nTuple to a numpy array and write everything to a file. This will take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeArrays(\"/mn/felt/u1/eirikgr/Downloads/TMVA_tree.root\", \"TMVA_tree\", \n",
    "            susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_all_bkg\",\"isSignal==0\", 100000)\n",
    "writeArrays(\"/mn/felt/u1/eirikgr/Downloads/TMVA_tree.root\", \"TMVA_tree\", \n",
    "            susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_all_sig\",\"isSignal==1\", 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some features on a small file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eventFile_bkg.close()\n",
    "#eventFile_sig.close()\n",
    "writeArrays(\"small.root\", \"TMVA_tree\", \n",
    "            susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_test_bkg\",\"isSignal==0\", 100)\n",
    "writeArrays(\"small.root\", \"TMVA_tree\", \n",
    "            susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_test_sig\",\"isSignal==1\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that some of these variables should not be used as discriminating variables (e.g. using the isSignal variable would not be fair to include :-)). Also the wgt, massSplit, dsid and isBkg should not be used. We will remove these later, before we start using the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = buildArraysFromROOT(tree,susyFeaturesNtup,\"\",0,100,\"ALL\")\n",
    "X[0]\n",
    "eventFile = h5py.File(\"/scratch3/eirikgr/ML_files/variables_test.h5\",\"r\")\n",
    "print eventFile.attrs['var_names']\n",
    "X_test = eventFile[\"TMVA_tree\"]\n",
    "#susyFeaturesNtup.remove(\"massSplit\")\n",
    "#susyFeaturesNtup.remove(\"wgt\")\n",
    "#susyFeaturesNtup.remove(\"isBkg\")\n",
    "#susyFeaturesNtup.remove(\"dsid\")\n",
    "#eventFile.close()\n",
    "print X_test[:].shape\n",
    "toplot = X_test[:,49]\n",
    "print toplot.shape\n",
    "print len(toplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(toplot,bins=100);\n",
    "plt.xlim(0,4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, before we start playing, let's plot the transverse momentum of the hardest lepton (i.e. lepton with the highest transverse momentum in the event) in each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "canvas = TCanvas(\"canvas\",\"canvas\",1)\n",
    "h_lep1_pt = TH1F(\"h_lep1_pt\",\"p_{T}^{lep1};p_{T} [GeV];Number of entries\",200,0,2000)\n",
    "tree.Draw(\"lep_pT1/1000>>h_lep1_pt\")\n",
    "h_lep1_pt.Draw(\"hist\")\n",
    "canvas.SetLogy()\n",
    "canvas.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nTuple contains a mix of signal and background samples. These can be identified by plotting the isSignal variable (equals 1 if event is signal, 0 if background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "canvas_2 = TCanvas(\"canvas_2\",\"canvas_2\",1)\n",
    "h_isSignal = TH1F(\"h_isSignal\",\"isSignal;;Number of entries\",2,0,2)\n",
    "h_isSignal.GetXaxis().SetBinLabel(1,\"Background\")\n",
    "h_isSignal.GetXaxis().SetBinLabel(2,\"SUSY signal\")\n",
    "tree.Draw(\"isSignal>>h_isSignal\")\n",
    "h_isSignal.Draw(\"hist\")\n",
    "canvas_2.SetLogy()\n",
    "canvas_2.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several signal samples in the nTuple (different SUSY scenarios). These are simplified models which means that they are basically simulations of a single feynman grap (see <a href=https://cds.cern.ch/record/2233683/files/ATL-COM-PHYS-2016-1673.pdf>here</a> for more details). The nTuple used here all signal samples are of the following kind\n",
    "<img src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/SUSY/FeynmanGraphs/C1C1-llvvN1N1-slsnu.png\",width=160,height=160>\n",
    "where the only difference between the various signal models is the mass difference between the chargino and the neutralino. Let's look at the values for the mass splitting for the available signal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "canvas_3 = TCanvas(\"canvas_3\",\"canvas_3\",1)\n",
    "h_massSplit = TH1F(\"h_massSplit\",\"M(#chi_{1})-M(#chi_{0});Mass split [GeV];Number of entries\",400,0,800)\n",
    "tree.Draw(\"massSplit>>h_massSplit\", \"isSignal==1\")\n",
    "h_massSplit.Draw(\"hist\")\n",
    "#canvas_3.Draw()\n",
    "canvas_8 = TCanvas(\"canvas_8\",\"canvas_8\",1)\n",
    "h_massSplit_dsid = TH2F(\"h_massSplit_dsid\",\"M(#chi_{1}^{#pm})-M(#chi_{1}^{0});Mass split [GeV];Data set ID\",400,0,800,22,392500,392521)\n",
    "tree.Draw(\"dsid:massSplit>>h_massSplit_dsid\", \"isSignal==1\")\n",
    "h_massSplit_dsid.Draw(\"\")\n",
    "canvas_8.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to divide background from signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selections\n",
    "cutBackground = \"isSignal==0\"\n",
    "cutSignal = \"isSignal==1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to single out the event weight which makes sure all the background samples are scaled to the same luminosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "susyWeightsNtup = [ \"wgt\" ]\n",
    "susyIdentifierNtup = [ \"isSignal\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatic numbers of event \n",
    "tcut = TCut(cutSignal)\n",
    "tree.Draw(\">>eventList\",tcut)\n",
    "eventList = TEventList()\n",
    "eventList = gDirectory.Get(\"eventList\")\n",
    "nSignalEvents = eventList.GetN()/2\n",
    "#nSignalEvents = 20000\n",
    "nBackgroundEvents = nSignalEvents\n",
    "print \"Will use %i events for signal and %i for background\" %(nSignalEvents,nBackgroundEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#canvas_4 = TCanvas(\"canvas_4\",\"canvas_4\",1)\n",
    "#h_MET_signal = TH1F(\"h_MET_signal\",\"Missing Transverse Energy;Missing Transverse Energy [GeV];Number of entries\",500,0,1000)\n",
    "#h_MET_backgr = TH1F(\"h_MET_backgr\",\"Missing Transverse Energy;Missing Transverse Energy [GeV];Number of entries\",500,0,1000)\n",
    "#tree.Draw(\"met/1000.>>h_MET_signal\", cutSignal)\n",
    "#tree.Draw(\"met/1000.>>h_MET_backgr\", cutBackground)\n",
    "#h_MET_signal.SetLineColor(kRed)\n",
    "#h_MET_signal.Scale(1./h_MET_signal.Integral())\n",
    "#h_MET_backgr.Scale(1./h_MET_backgr.Integral())\n",
    "#h_MET_backgr.Draw(\"hist\")\n",
    "#h_MET_signal.Draw(\"hist same\")\n",
    "#canvas_4.SetLogy()\n",
    "#canvas_4.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#canvas_5 = TCanvas(\"canvas_5\",\"canvas_5\",1)\n",
    "#h_MEFF_MET_signal = TH2F(\"h_MEFF_MET_signal\",\"Missing Transverse Energy vs. M_{eff};Missing Transverse Energy [GeV];M_{eff} [GeV]\",500,0,1000,500,0,1000)\n",
    "#h_MEFF_MET_backgr = TH2F(\"h_MEFF_MET_backgr\",\"Missing Transverse Energy vs. M_{eff};Missing Transverse Energy [GeV];M_{eff} [GeV]\",500,0,1000,500,0,1000)\n",
    "#tree.Draw(\"met/1000.:meff/1000.>>h_MEFF_MET_signal\", cutSignal)\n",
    "#tree.Draw(\"met/1000.:meff/1000.>>h_MEFF_MET_backgr\", cutBackground)\n",
    "#h_MET_signal.SetMarkerColor(kRed)\n",
    "#h_MET_signal.Scale(1./h_MET_signal.Integral())\n",
    "#h_MET_backgr.Scale(1./h_MET_backgr.Integral())\n",
    "#h_MET_backgr.Draw(\"\")\n",
    "#h_MET_signal.Draw(\"\")\n",
    "#canvas_5.SetLogy()\n",
    "#canvas_5.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build data arrays\n",
    "# Background\n",
    "X_train_bg = buildArraysFromROOT(tree,susyFeaturesNtup,cutBackground,0,nBackgroundEvents,\"TRAINING SAMPLE (background)\")\n",
    "X_test_bg = buildArraysFromROOT(tree,susyFeaturesNtup,cutBackground,nBackgroundEvents,nBackgroundEvents,\"TESTING SAMPLE (background)\")\n",
    "\n",
    "# Signal\n",
    "X_train_sig = buildArraysFromROOT(tree,susyFeaturesNtup,cutSignal,0,nSignalEvents,\"TRAINING SAMPLE (signal)\")\n",
    "X_test_sig = buildArraysFromROOT(tree,susyFeaturesNtup,cutSignal,nSignalEvents,nSignalEvents,\"TESTING SAMPLE (signal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeArrays(, selection = \"\", chunkSize = 1000000)\n",
    "X_train_bg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writeArrays(\"/mn/felt/u1/eirikgr/Downloads/TMVA_tree.root\", \"TMVA_tree\", \n",
    "            #susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_background\",cutBackground, 1000000)\n",
    "#X_bkg = buildArraysFromROOT(tree,susyFeaturesNtup,cutBackground,0,50,\"Variable\")\n",
    "#Y_bkg = buildArraysFromROOT(tree,susyIdentifierNtup,cutBackground,0,50,\"Signal or no signal\")\n",
    "#X_sig = buildArraysFromROOT(tree,susyFeaturesNtup,cutSignal,0,50,\"Variable\")\n",
    "#Y_sig = buildArraysFromROOT(tree,susyIdentifierNtup,cutSignal,0,50,\"Signal or no signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writeArrays(\"/mn/felt/u1/eirikgr/Downloads/TMVA_tree.root\", \"TMVA_tree\", \n",
    "            #susyFeaturesNtup, \"/scratch3/eirikgr/ML_files/variables_signal\",cutSignal, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to read the numpy array from file and remove the variables we don't want to include as disriminating variables in our fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backgroundFile = h5py.File(\"/scratch3/eirikgr/ML_files/variables_background.h5\",\"r\")\n",
    "#signalFile     = h5py.File(\"/scratch3/eirikgr/ML_files/variables_signal.h5\",\"r\")\n",
    "#eventFile = h5py.File(\"/scratch3/eirikgr/ML_files/variables_all.h5\",\"r\")\n",
    "#print type(eventFile)\n",
    "eventFile_bkg = h5py.File(\"/scratch3/eirikgr/ML_files/variables_test_bkg.h5\",\"r\")\n",
    "eventFile_sig = h5py.File(\"/scratch3/eirikgr/ML_files/variables_test_sig.h5\",\"r\")\n",
    "attributes = eventFile_bkg.attrs[\"var_names\"]\n",
    "#print attributes.shape\n",
    "susyFeaturesExclude = {}\n",
    "susyFeaturesExclude[\"massSplit\"] = -1\n",
    "susyFeaturesExclude[\"wgt\"] = -1\n",
    "susyFeaturesExclude[\"isBkg\"] = -1\n",
    "susyFeaturesExclude[\"isSignal\"] = -1\n",
    "susyFeaturesExclude[\"dsid\"] = -1\n",
    "#print itemindex[0][0]\n",
    "delitems = []\n",
    "for sfe in susyFeaturesExclude.keys():\n",
    "    itemindex = np.where(attributes==sfe)\n",
    "    print \"Removing variable %-10s in index %i\" %(sfe,itemindex[0][0])\n",
    "    delitems.append(itemindex[0][0])\n",
    "    susyFeaturesExclude[sfe] = itemindex[0][0]\n",
    "# get the full numpy array\n",
    "all_var_bkg = eventFile_bkg[\"TMVA_tree\"]\n",
    "all_var_sig = eventFile_sig[\"TMVA_tree\"]\n",
    "print \"\\n\"\n",
    "print \"There are a total of %i events in the background nTuple\" %(all_var_bkg.shape[0])\n",
    "print \"There are a total of %i events in the signal nTuple\" %(all_var_sig.shape[0])\n",
    "print \"\\n\"\n",
    "# get the signal/no-signal vector\n",
    "Y_bkg = all_var_bkg[:,susyFeaturesExclude[\"isSignal\"]]\n",
    "Y_sig = all_var_sig[:,susyFeaturesExclude[\"isSignal\"]]\n",
    "# get the weights\n",
    "W_bkg = all_var_bkg[:,susyFeaturesExclude[\"wgt\"]]\n",
    "W_sig = all_var_sig[:,susyFeaturesExclude[\"wgt\"]]\n",
    "# get the numpy array without the unwanted variables\n",
    "X_bkg = np.delete(all_var_bkg,delitems,1)\n",
    "X_sig = np.delete(all_var_sig,delitems,1)\n",
    "\n",
    "print \"Backgrounds are now stored in X_bkg with %i entries and %i variables\" %(X_bkg.shape[0],X_bkg.shape[1])\n",
    "print \"Signals     are now stored in X_sig with %i entries and %i variables\" %(X_sig.shape[0],X_sig.shape[1])\n",
    "print \"\\n\"\n",
    "print \"Target  for background is now stored in Y_bkg with %i entries\" %(Y_bkg.shape[0])\n",
    "print \"Weights for background are now stored in W_bkg with %i entries\" %(W_bkg.shape[0])\n",
    "print \"\\n\"\n",
    "print \"Target  for signals is now stored in Y_sig with %i entries\" %(Y_sig.shape[0])\n",
    "print \"Weights for signals are now stored in W_sig with %i entries\" %(W_sig.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of events for each data set ID (dsid). We want to train on all the various backgrounds and signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eventFile_bkg = h5py.File(\"/scratch3/eirikgr/ML_files/variables_all_bkg.h5\",\"r\")\n",
    "eventFile_sig = h5py.File(\"/scratch3/eirikgr/ML_files/variables_all_sig.h5\",\"r\")\n",
    "attributes = eventFile_sig.attrs[\"var_names\"]\n",
    "#print attributes.shape\n",
    "susyFeaturesExclude = {}\n",
    "susyFeaturesExclude[\"massSplit\"] = -1\n",
    "susyFeaturesExclude[\"wgt\"] = -1\n",
    "susyFeaturesExclude[\"isBkg\"] = -1\n",
    "susyFeaturesExclude[\"isSignal\"] = -1\n",
    "susyFeaturesExclude[\"dsid\"] = -1\n",
    "#print itemindex[0][0]\n",
    "delitems = []\n",
    "for sfe in susyFeaturesExclude.keys():\n",
    "    itemindex = np.where(attributes==sfe)\n",
    "    print \"Unwanted variable %-10s in index %i\" %(sfe,itemindex[0][0])\n",
    "    delitems.append(itemindex[0][0])\n",
    "    susyFeaturesExclude[sfe] = itemindex[0][0]\n",
    "# get the full numpy array\n",
    "all_var_bkg = eventFile_bkg[\"TMVA_tree\"]\n",
    "all_var_sig = eventFile_sig[\"TMVA_tree\"]\n",
    "print \"\\n\"\n",
    "print \"There are a total of %i events in the background nTuple\" %(all_var_bkg.shape[0])\n",
    "print \"There are a total of %i events in the signal nTuple\" %(all_var_sig.shape[0])\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce the data set to train on. Should be a mix of backgrounds and signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rBackgroundEvents = 0.2\n",
    "rSignalEvents = 0.2\n",
    "X_train_bkg = {}\n",
    "X_train_sig = {}\n",
    "X_test_bkg  = {}\n",
    "X_test_sig  = {}\n",
    "W_train_bkg = {}\n",
    "W_train_sig = {}\n",
    "W_test_bkg  = {}\n",
    "W_test_sig  = {}\n",
    "counts_dsids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsids = np.concatenate((all_var_bkg[:,susyFeaturesExclude[\"dsid\"]],all_var_sig[:,susyFeaturesExclude[\"dsid\"]]),axis=0)\n",
    "print \"There are %i unique data sets in the signal and background samples\" %np.unique(dsids).shape[0]\n",
    "n = 0\n",
    "for dsid in np.unique(dsids):\n",
    "    #\n",
    "    if (dsid >= 392500 and dsid <= 392521): continue\n",
    "    print \"Doing dsid %i/%i\" %(n,np.unique(dsids).shape[0])\n",
    "    num = 0\n",
    "    isBkg = True\n",
    "    indexs = np.where(all_var_bkg[:,susyFeaturesExclude[\"dsid\"]] == dsid)\n",
    "    num = indexs[0].shape[0]\n",
    "    if not num: \n",
    "        indexs = np.where(all_var_sig[:,susyFeaturesExclude[\"dsid\"]] == dsid)\n",
    "        num = indexs[0].shape[0]\n",
    "        isBkg = False\n",
    "    if num < 5: \n",
    "        print \"Skipping %i since only %i events\" %(dsid,num)\n",
    "        continue\n",
    "    if isBkg:\n",
    "        counts_dsids[dsid] = num\n",
    "        X_train_bkg[dsid] = all_var_bkg[indexs[0][:int(math.ceil(num*rBackgroundEvents))],:]\n",
    "        X_test_bkg[dsid]  = all_var_bkg[indexs[0][int(math.ceil(num*rBackgroundEvents)):],:]\n",
    "    else:   \n",
    "        num = indexs[0].shape[0]\n",
    "        print num\n",
    "        counts_dsids[dsid] = num\n",
    "        X_train_sig[dsid] = all_var_sig[indexs[0][:int(math.ceil(num*rSignalEvents))],:]\n",
    "        X_test_sig[dsid]  = all_var_sig[indexs[0][int(math.ceil(num*rSignalEvents)):],:]\n",
    "        print X_train_sig[dsid].shape[0]\n",
    "    n += 1\n",
    "        \n",
    "for dsid in sorted(counts_dsids.keys()):\n",
    "    if dsid in X_train_bkg.keys():\n",
    "        W_train_bkg[dsid] = X_train_bkg[dsid][:,susyFeaturesExclude[\"wgt\"]]\n",
    "        W_test_bkg[dsid]  = X_test_bkg[dsid][:,susyFeaturesExclude[\"wgt\"]]\n",
    "        print \"DSID %i has %i events and %i (%i) events used for training (testing)\" %(int(dsid),counts_dsids[dsid],X_train_bkg[dsid].shape[0],X_test_bkg[dsid].shape[0])\n",
    "    elif dsid in X_train_sig.keys():\n",
    "        W_train_sig[dsid] = X_train_sig[dsid][:,susyFeaturesExclude[\"wgt\"]]\n",
    "        W_test_sig[dsid]  = X_test_sig[dsid][:,susyFeaturesExclude[\"wgt\"]]\n",
    "        print \"DSID %i has %i events and %i (%i) events used for training (testing)\" %(int(dsid),counts_dsids[dsid],X_train_sig[dsid].shape[0],X_test_sig[dsid].shape[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_train = np.array([])\n",
    "X_train = np.array([])\n",
    "W_test  = np.array([])\n",
    "X_test  = np.array([])\n",
    "n = 1\n",
    "for dsid in sorted(counts_dsids.keys()):\n",
    "    if dsid in X_train_bkg.keys():\n",
    "        if X_train.shape[0] == 0:\n",
    "            X_train = np.copy(X_train_bkg[dsid])\n",
    "            W_train = np.copy(W_train_bkg[dsid])\n",
    "            X_test = np.copy(X_test_bkg[dsid])\n",
    "            W_test = np.copy(W_test_bkg[dsid])\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train_bkg[dsid],X_train),axis=0)\n",
    "            W_train = np.concatenate((W_train_bkg[dsid],W_train),axis=0)\n",
    "            X_test  = np.concatenate((X_test_bkg[dsid],X_test),axis=0)\n",
    "            W_test  = np.concatenate((W_test_bkg[dsid],W_test),axis=0)\n",
    "        print \"Adding %i/%i with %i events to training sample. Now is %i\" %(n,len(counts_dsids.keys()),X_train_bkg[dsid].shape[0],X_train.shape[0])\n",
    "    elif dsid in X_train_sig.keys():\n",
    "        if X_train.shape[0] == 0:\n",
    "            X_train = np.copy(X_train_sig[dsid])\n",
    "            W_train = np.copy(W_train_sig[dsid])\n",
    "            X_test  = np.copy(X_test_sig[dsid])\n",
    "            W_test  = np.copy(W_test_sig[dsid])\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train_sig[dsid],X_train),axis=0)\n",
    "            W_train = np.concatenate((W_train_sig[dsid],W_train),axis=0)\n",
    "            X_test  = np.concatenate((X_test_sig[dsid],X_test),axis=0)\n",
    "            W_test  = np.concatenate((W_test_sig[dsid],W_test),axis=0)\n",
    "        print \"Adding %i/%i with %i events to training sample. Now is %i\" %(n,len(counts_dsids.keys()),X_train_sig[dsid].shape[0],X_train.shape[0])\n",
    "   \n",
    "        \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check that the shapes match and make the target vector (Y)\n",
    "print W_train.shape\n",
    "print X_train.shape\n",
    "Y_train = np.where(X_train[:,susyFeaturesExclude[\"isSignal\"]],1,0)\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check that the shapes match and make the target vector (Y)\n",
    "print W_test.shape\n",
    "print X_test.shape\n",
    "Y_test = np.where(X_test[:,susyFeaturesExclude[\"isSignal\"]],1,0)\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_clean = np.delete(X_train,delitems,1)\n",
    "X_test_clean = np.delete(X_test,delitems,1)\n",
    "print X_test_clean.shape\n",
    "print \"TRAIN sample: Removed %i variables (%s), shape is now (%s,%s)\" %(len(delitems),\",\".join(susyFeaturesExclude),X_train_clean.shape[0],X_train_clean.shape[1])\n",
    "print \"TEST  sample: Removed %i variables (%s), shape is now (%s,%s)\" %(len(delitems),\",\".join(susyFeaturesExclude),X_test_clean.shape[0],X_test_clean.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 2\n",
    "lambda_=1e-5\n",
    "logreg = linear_model.LogisticRegression(C=1/lambda_,max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg.fit(X_train_clean, Y_train.ravel(),W_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_coeff = np.sort(logreg.coef_[0])\n",
    "i = 1\n",
    "data = np.arange(len(sorted_coeff),dtype=\"f\")\n",
    "label = np.arange(len(sorted_coeff),dtype='object')\n",
    "for sc in reversed(sorted_coeff):\n",
    "    itemindex = np.where(logreg.coef_[0]==sc)\n",
    "    label[i-1] = attributes[itemindex[0][0]]\n",
    "    data[i-1] = float(sc)\n",
    "    i += 1\n",
    "plt.figure(figsize=(38,20))\n",
    "plt.xticks(rotation='vertical',fontsize=42)\n",
    "plt.bar(np.arange(i-1),data,tick_label=label,bottom=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_score(X,Y,logreg):\n",
    "    predictions=logreg.predict(X)\n",
    "    print('Accuracy:', accuracy_score(Y, predictions))\n",
    "    print('Precision:', precision_score(Y, predictions, average='macro'))\n",
    "    print('Recall:', recall_score(Y, predictions, average='macro'))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = print_score(X_train_clean, Y_train,logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_clean = np.delete(X_test,delitems,1)\n",
    "print_score(X_test_clean, Y_test,logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = logreg.predict(X_train)\n",
    "dsid      = np.concatenate((all_var_bkg[:,50],all_var_sig[:,50]),axis=0)\n",
    "masssplit = np.concatenate((all_var_bkg[:,53],all_var_sig[:,53]),axis=0)\n",
    "wgt       = np.concatenate((W_bkg,W_sig),axis=0)\n",
    "predRight = 0.\n",
    "predWrong = 0.\n",
    "yields    = {}\n",
    "label = np.arange(len(sorted_coeff),dtype='object')\n",
    "for i in range(len(pred)):\n",
    "    if not dsid[i] in yields.keys():\n",
    "        yields[dsid[i]] = {\"nbkg\":0,\"nsig\":0}\n",
    "    if pred[i]: yields[dsid[i]][\"nsig\"] += wgt[i]\n",
    "    else: yields[dsid[i]][\"nbkg\"] += wgt[i]\n",
    "    #print \"Predicted %i, true is %i\" %(pred[i],Y_test[i])\n",
    "    if pred[i] == Y_train[i]:\n",
    "        predRight += 1.\n",
    "    else:\n",
    "        predWrong += 1.\n",
    "print \"Got it right in %.2f of the cases!\" %(predRight/float(i))\n",
    "print yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_MET_sig = TH1F(\"h_MET_sig\",\"\",100,0,1000)\n",
    "h_MET_bkg = TH1F(\"h_MET_bkg\",\"\",100,0,1000)\n",
    "n = 0\n",
    "for p in pred:\n",
    "    if p == 0:\n",
    "        h_MET_bkg.Fill(X_test[n][19]/1000.)\n",
    "    if p == 1:\n",
    "        h_MET_sig.Fill(X_test[n][19]/1000.)\n",
    "    n += 1\n",
    "    \n",
    "canvas_6 = TCanvas(\"canvas_6\",\"canvas_6\",1)\n",
    "h_MET_sig.SetLineColor(kRed)\n",
    "h_MET_bkg.Draw(\"hist\")\n",
    "h_MET_sig.Draw(\"same hist\")\n",
    "canvas_6.SetLogy()\n",
    "canvas_6.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_boundary(X,y,degree,logreg):\n",
    "    x_min, x_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    y_min, y_max = X[:, 2].min() - .5, X[:, 2].max() + .5\n",
    "    xs, ys = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "    xys = mapFeature(xs.ravel(),ys.ravel(),degree)\n",
    "    Z = logreg.predict(xys).reshape(xs.shape)\n",
    "    plt.pcolormesh(xs, ys, Z, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X[:, 1], X[:, 2], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.xlim(xs.min(), xs.max())\n",
    "    plt.ylim(ys.min(), ys.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2, degree):\n",
    "    out = np.ones(( X1.shape[0], sum(range(degree + 2)) )) # could also use ((degree+1) * (degree+2)) / 2 instead of sum\n",
    "    curr_column = 1\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i+1):\n",
    "            out[:,curr_column] = np.power(X1,i-j) * np.power(X2,j)\n",
    "            curr_column += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt=plot_boundary(X,Y,degree,logreg)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
